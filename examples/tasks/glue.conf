[defaults]

# Model architecture for textcat
textcat_arch = softmax_class_vector

# The maximum total input sequence length after tokenization.
max_seq_length = 128

# Dropout rate
dropout = 0.1

# Number of examples per training batch
batch_size = 32

# Accumulate gradients over this many steps to make one batch
steps_per_batch = 2

# Number of examples per eval batch
eval_batch_size = 32

# The initial learning rate for Adam
learning_rate = 2e-5

# Learning rate for the output classifier
classifier_lr = 0.001

# Weight decay (aka L2 penalty)
weight_decay = 0.0

# Epsilon for the Adam optimizer
adam_epsilon = 1e-8

# Max gradient norm (aka gradient clipping)
max_grad_norm = 1.0

# Whether to use the learning rate schedule (or keep LR fixed)
use_learn_rate_schedule = 1

# How wide to make the triangular LR cycle. From lr/lr_range to lr*lr_range.
lr_range = 2

# How many epochs to set as the period cycle for the cyclic learning rate.
lr_period = 2

# Whether to use stochastic weight averaging
use_swa = 1

# Set random seed
seed = 0

# How often (by steps) to evaluate
eval_every = 100

# How many checkpoints to elapse without improvement before stopping
patience = 10

# Unused values
num_train_epochs = -1
max_steps = -1
warmup_steps = -1
